#!/usr/bin/env python3
"""
VIBEZEN æœ€é©åŒ–å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼
é«˜é€ŸåŒ–ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’é‡è¦–ã—ãŸå®Ÿè£…

æ©Ÿèƒ½:
- ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç†
- æ§‹é€ åŒ–ã•ã‚ŒãŸå“è³ªå•é¡Œãƒ‡ãƒ¼ã‚¿
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç®¡ç†
- è©³ç´°ãªå“è³ªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ”¹å–„:
- å…·ä½“çš„ä¾‹å¤–å‡¦ç†
- ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ”¹å–„
"""

import sys
import ast
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from dataclasses import dataclass
import json

@dataclass
class QualityIssue:
    """å“è³ªå•é¡Œãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    file: str
    line: int
    issue_type: str
    description: str
    severity: str = "medium"

@dataclass
class FileAnalysisResult:
    """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    file: str
    total_lines: int
    code_lines: int
    issues: List[QualityIssue]
    analysis_time: float
    error: Optional[str] = None

class OptimizedQualityChecker:
    """æœ€é©åŒ–å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼"""
    
    # å®šæ•°å®šç¾©
    DEFAULT_MAX_WORKERS = 4
    DEFAULT_FILE_SIZE_LIMIT = 1_000_000  # 1MB
    LONG_LINE_THRESHOLD = 120
    MAX_FUNCTION_LINES = 50
    MAX_LONG_LINE_ISSUES = 20
    MAX_MAGIC_NUMBER_ISSUES = 15
    FILE_TIMEOUT_SECONDS = 2.0
    
    def __init__(self, max_workers: int = None, file_size_limit: int = None):
        self.max_workers = max_workers or self.DEFAULT_MAX_WORKERS
        self.file_size_limit = file_size_limit or self.DEFAULT_FILE_SIZE_LIMIT
        self.magic_number_pattern = re.compile(r'\b(?<!\.)\d{2,}\b(?!\.)(?!\w)')
        self.long_line_threshold = self.LONG_LINE_THRESHOLD
        self.max_function_lines = self.MAX_FUNCTION_LINES
        
    def is_file_too_large(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯"""
        try:
            return file_path.stat().st_size > self.file_size_limit
        except (OSError, IOError, PermissionError):
            return True
    
    def analyze_file_fast(self, file_path: Path) -> FileAnalysisResult:
        """é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"""
        start_time = time.perf_counter()
        
        try:
            # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if self.is_file_too_large(file_path):
                return FileAnalysisResult(
                    file=file_path.name,
                    total_lines=0,
                    code_lines=0,
                    issues=[],
                    analysis_time=time.perf_counter() - start_time,
                    error="File too large (>1MB)"
                )
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            lines = content.split('\n')
            total_lines = len(lines)
            code_lines = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
            
            issues = []
            
            # é«˜é€Ÿå“è³ªãƒã‚§ãƒƒã‚¯
            self._check_long_lines(lines, issues, file_path.name)
            self._check_magic_numbers(lines, issues, file_path.name)
            self._check_function_length_fast(content, issues, file_path.name)
            
            analysis_time = time.perf_counter() - start_time
            
            return FileAnalysisResult(
                file=file_path.name,
                total_lines=total_lines,
                code_lines=code_lines,
                issues=issues,
                analysis_time=analysis_time
            )
            
        except (UnicodeDecodeError, IOError, PermissionError) as e:
            return FileAnalysisResult(
                file=file_path.name,
                total_lines=0,
                code_lines=0,
                issues=[],
                analysis_time=time.perf_counter() - start_time,
                error=f'File access error: {str(e)}'
            )
        except Exception as e:
            return FileAnalysisResult(
                file=file_path.name,
                total_lines=0,
                code_lines=0,
                issues=[],
                analysis_time=time.perf_counter() - start_time,
                error=f'Unexpected analysis error: {str(e)}'
            )
    
    def _check_long_lines(self, lines: List[str], issues: List[QualityIssue], filename: str):
        """é•·ã„è¡Œãƒã‚§ãƒƒã‚¯ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        for i, line in enumerate(lines, 1):
            if len(line) > self.long_line_threshold:
                issues.append(QualityIssue(
                    file=filename,
                    line=i,
                    issue_type="long_line",
                    description=f"é•·ã„è¡Œ ({len(line)}æ–‡å­—)",
                    severity="low"
                ))
                
                # å¤§é‡ã®é•·ã„è¡ŒãŒã‚ã‚‹å ´åˆã¯æ—©æœŸçµ‚äº†
                if len([iss for iss in issues if iss.issue_type == "long_line"]) > self.MAX_LONG_LINE_ISSUES:
                    break
    
    def _check_magic_numbers(self, lines: List[str], issues: List[QualityIssue], filename: str):
        """ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        magic_count = 0
        for i, line in enumerate(lines, 1):
            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
            if line.strip().startswith('#'):
                continue
                
            # æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«å†…ã¯é™¤å¤–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            if '"' in line or "'" in line:
                continue
                
            numbers = self.magic_number_pattern.findall(line)
            for num in numbers:
                # ä¸€èˆ¬çš„ãªå€¤ã¯é™¤å¤–
                if num in ['10', '20', '30', '50', '100', '200', '500', '1000']:
                    continue
                    
                issues.append(QualityIssue(
                    file=filename,
                    line=i,
                    issue_type="magic_number",
                    description=f"ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ '{num}'",
                    severity="medium"
                ))
                
                magic_count += 1
                # å¤§é‡ã®ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ãŒã‚ã‚‹å ´åˆã¯æ—©æœŸçµ‚äº†
                if magic_count > self.MAX_MAGIC_NUMBER_ISSUES:
                    return
    
    def _check_function_length_fast(self, content: str, issues: List[QualityIssue], filename: str):
        """é–¢æ•°é•·ãƒã‚§ãƒƒã‚¯ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
        try:
            # ç°¡æ˜“çš„ãªé–¢æ•°æ¤œå‡ºï¼ˆASTè§£æã‚ˆã‚Šé«˜é€Ÿï¼‰
            lines = content.split('\n')
            in_function = False
            function_start = 0
            function_name = ""
            indent_level = 0
            
            for i, line in enumerate(lines, 1):
                stripped = line.strip()
                
                # é–¢æ•°å®šç¾©æ¤œå‡º
                if stripped.startswith('def ') and ':' in stripped:
                    if in_function and i - function_start > self.max_function_lines:
                        issues.append(QualityIssue(
                            file=filename,
                            line=function_start,
                            issue_type="long_function",
                            description=f"é•·ã„é–¢æ•° '{function_name}' ({i - function_start}è¡Œ)",
                            severity="high"
                        ))
                    
                    in_function = True
                    function_start = i
                    function_name = stripped.split('(')[0].replace('def ', '')
                    indent_level = len(line) - len(line.lstrip())
                
                # é–¢æ•°çµ‚äº†æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
                elif in_function and stripped and len(line) - len(line.lstrip()) <= indent_level and not line.startswith(' '):
                    if i - function_start > self.max_function_lines:
                        issues.append(QualityIssue(
                            file=filename,
                            line=function_start,
                            issue_type="long_function",
                            description=f"é•·ã„é–¢æ•° '{function_name}' ({i - function_start}è¡Œ)",
                            severity="high"
                        ))
                    in_function = False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«çµ‚äº†æ™‚ã®æœ€å¾Œã®é–¢æ•°ãƒã‚§ãƒƒã‚¯
            if in_function and len(lines) - function_start > self.max_function_lines:
                issues.append(QualityIssue(
                    file=filename,
                    line=function_start,
                    issue_type="long_function",
                    description=f"é•·ã„é–¢æ•° '{function_name}' ({len(lines) - function_start}è¡Œ)",
                    severity="high"
                ))
                
        except (UnicodeDecodeError, ValueError) as e:
            # é–¢æ•°é•·ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶™ç¶š
            pass
    
    def analyze_project_optimized(self, project_path: str, max_files: int = 200) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæœ€é©åŒ–åˆ†æ"""
        start_time = time.perf_counter()
        
        print(f"ğŸ” æœ€é©åŒ–å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹: {Path(project_path).name}")
        print("=" * 50)
        
        # Pythonãƒ•ã‚¡ã‚¤ãƒ«åé›†ï¼ˆåˆ¶é™ä»˜ãï¼‰
        project_dir = Path(project_path)
        python_files = list(project_dir.rglob("*.py"))
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™
        if len(python_files) > max_files:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™: {len(python_files)} â†’ {max_files}ä»¶")
            python_files = python_files[:max_files]
        
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(python_files)}ä»¶")
        
        # ä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–
        results = []
        total_issues = 0
        total_lines = 0
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚’ãƒãƒƒãƒã§å®Ÿè¡Œ
            future_to_file = {
                executor.submit(self.analyze_file_fast, file_path): file_path 
                for file_path in python_files
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result(timeout=self.FILE_TIMEOUT_SECONDS)
                    results.append(result)
                    
                    if result.error:
                        print(f"âš ï¸ {result.file}: {result.error}")
                    else:
                        total_lines += result.code_lines
                        issue_count = len(result.issues)
                        total_issues += issue_count
                        
                        if issue_count > 0:
                            print(f"âš ï¸ {result.file}: {issue_count}ä»¶")
                        else:
                            print(f"âœ… {result.file}: å•é¡Œãªã—")
                            
                except TimeoutError:
                    print(f"â° {file_path.name}: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (2ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ)")
                except Exception as e:
                    print(f"âŒ {file_path.name}: å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {str(e)}")
        
        # åˆ†ææ™‚é–“
        analysis_time = time.perf_counter() - start_time
        
        # çµæœã‚µãƒãƒªãƒ¼
        self._print_summary(total_lines, total_issues, analysis_time, len(python_files))
        
        return {
            'total_files': len(python_files),
            'total_lines': total_lines,
            'total_issues': total_issues,
            'analysis_time': analysis_time,
            'throughput': len(python_files) / analysis_time if analysis_time > 0 else 0,
            'results': results
        }
    
    def _print_summary(self, total_lines: int, total_issues: int, analysis_time: float, file_count: int):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "=" * 50)
        print("ğŸ“Š æœ€é©åŒ–å“è³ªãƒã‚§ãƒƒã‚¯çµæœ")
        print("=" * 50)
        
        issue_density = (total_issues / total_lines) * 1000 if total_lines > 0 else 0
        throughput = file_count / analysis_time if analysis_time > 0 else 0
        
        print(f"\nğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"  åˆ†ææ™‚é–“: {analysis_time:.3f}ç§’")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} files/sec")
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
        
        print(f"\nğŸ“Š å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        print(f"  ç·ã‚³ãƒ¼ãƒ‰è¡Œæ•°: {total_lines:,}è¡Œ")
        print(f"  æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {total_issues}ä»¶")
        print(f"  å•é¡Œå¯†åº¦: {issue_density:.1f}ä»¶/1000è¡Œ")
        
        # è©•ä¾¡
        if issue_density < 5:
            grade = "A"
            emoji = "ğŸŒŸ"
            comment = "å„ªç§€ï¼éå¸¸ã«é«˜å“è³ªãªã‚³ãƒ¼ãƒ‰ã§ã™"
        elif issue_density < 10:
            grade = "B"
            emoji = "âœ…"
            comment = "è‰¯å¥½ã€‚ä¸€éƒ¨æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™"
        elif issue_density < 20:
            grade = "C"
            emoji = "ğŸŸ¡"
            comment = "è¦æ³¨æ„ã€‚å“è³ªæ”¹å–„ãŒå¿…è¦ã§ã™"
        else:
            grade = "D"
            emoji = "ğŸ”´"
            comment = "å•é¡Œã‚ã‚Šã€‚å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦ã§ã™"
        
        print(f"\nç·åˆè©•ä¾¡: {emoji} ã‚°ãƒ¬ãƒ¼ãƒ‰ {grade}")
        print(f"ã‚³ãƒ¡ãƒ³ãƒˆ: {comment}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        if throughput > 20:
            perf_grade = "ğŸš€ é«˜é€Ÿ"
        elif throughput > 10:
            perf_grade = "âš¡ æ¨™æº–"
        elif throughput > 5:
            perf_grade = "ğŸŒ ä½é€Ÿ"
        else:
            perf_grade = "ğŸ”¥ è¦æœ€é©åŒ–"
        
        print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {perf_grade} ({throughput:.1f} files/sec)")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    if len(sys.argv) > 1:
        project_path = sys.argv[1]
    else:
        project_path = Path.cwd()
    
    checker = OptimizedQualityChecker(max_workers=4)
    results = checker.analyze_project_optimized(str(project_path))
    
    # çµæœä¿å­˜
    output_file = Path(project_path) / "optimized_quality_report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        # QualityIssueã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        serializable_results = {
            'total_files': results['total_files'],
            'total_lines': results['total_lines'],
            'total_issues': results['total_issues'],
            'analysis_time': results['analysis_time'],
            'throughput': results['throughput'],
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")
    
    return results

if __name__ == "__main__":
    main()