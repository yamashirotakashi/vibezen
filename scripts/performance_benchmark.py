#!/usr/bin/env python3
"""
VIBEZEN ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
å„æ©Ÿèƒ½ã®å®Ÿè¡Œæ™‚é–“ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æ¸¬å®š
"""

import time
import psutil
import tracemalloc
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Tuple
import json
from datetime import datetime
import subprocess
import sys
import os

class VIBEZENBenchmark:
    """VIBEZENæ€§èƒ½æ¸¬å®šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.results: Dict[str, Any] = {}
        self.process = psutil.Process()
        
    def start_measurement(self, test_name: str) -> Dict[str, Any]:
        """æ¸¬å®šé–‹å§‹"""
        tracemalloc.start()
        start_time = time.perf_counter()
        start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        return {
            'name': test_name,
            'start_time': start_time,
            'start_memory': start_memory
        }
    
    def end_measurement(self, measurement: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸¬å®šçµ‚äº†"""
        end_time = time.perf_counter()
        end_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        result = {
            'name': measurement['name'],
            'execution_time': end_time - measurement['start_time'],
            'memory_usage': {
                'start_mb': measurement['start_memory'],
                'end_mb': end_memory,
                'peak_mb': peak / 1024 / 1024,
                'current_mb': current / 1024 / 1024
            },
            'timestamp': datetime.now().isoformat()
        }
        
        self.results[measurement['name']] = result
        return result
    
    def benchmark_quality_check(self, project_path: str) -> Dict[str, Any]:
        """å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸ” å“è³ªãƒã‚§ãƒƒã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {project_path}")
        
        measurement = self.start_measurement("quality_check")
        
        try:
            # å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            os.chdir(project_path)
            result = subprocess.run([
                sys.executable, "vibezen_quality_check.py"
            ], capture_output=True, text=True, timeout=30)
            
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['success'] = result.returncode == 0
            benchmark_result['output_lines'] = len(result.stdout.split('\n'))
            
            print(f"âœ… å®Ÿè¡Œæ™‚é–“: {benchmark_result['execution_time']:.3f}ç§’")
            print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {benchmark_result['memory_usage']['peak_mb']:.1f}MB")
            
            return benchmark_result
            
        except ValueError as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = f'Path validation error: {str(e)}'
            print(f"âŒ ãƒ‘ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return benchmark_result
        except (subprocess.TimeoutExpired, TimeoutError) as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = 'Execution timeout'
            print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")
            return benchmark_result
        except Exception as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = f'Unexpected error: {str(e)}'
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            return benchmark_result
    
    def benchmark_vz_command(self, project_path: str) -> Dict[str, Any]:
        """[VZ]ã‚³ãƒãƒ³ãƒ‰ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸš€ [VZ]ã‚³ãƒãƒ³ãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {project_path}")
        
        measurement = self.start_measurement("vz_command")
        
        try:
            # VZã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
            os.chdir(project_path)
            result = subprocess.run([
                sys.executable, 
                "/mnt/c/Users/tky99/dev/vibezen/scripts/simple_vz_test.py", 
                "vz"
            ], capture_output=True, text=True, timeout=60)
            
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['success'] = result.returncode == 0
            benchmark_result['output_lines'] = len(result.stdout.split('\n'))
            
            print(f"âœ… å®Ÿè¡Œæ™‚é–“: {benchmark_result['execution_time']:.3f}ç§’")
            print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {benchmark_result['memory_usage']['peak_mb']:.1f}MB")
            
            return benchmark_result
            
        except ValueError as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = f'Path validation error: {str(e)}'
            print(f"âŒ ãƒ‘ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return benchmark_result
        except (subprocess.TimeoutExpired, TimeoutError) as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = 'Execution timeout'
            print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")
            return benchmark_result
        except Exception as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = f'Unexpected error: {str(e)}'
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            return benchmark_result
    
    def benchmark_large_project(self, project_path: str, file_count_limit: int = 100) -> Dict[str, Any]:
        """å¤§è¦æ¨¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‡¦ç†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸ“ å¤§è¦æ¨¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {project_path}")
        
        measurement = self.start_measurement("large_project")
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚«ã‚¦ãƒ³ãƒˆ
            python_files = list(Path(project_path).rglob("*.py"))
            file_count = len(python_files[:file_count_limit])
            
            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
            start_analysis = time.perf_counter()
            
            issues_total = 0
            for py_file in python_files[:file_count_limit]:
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç°¡æ˜“è§£æï¼ˆè¡Œæ•°ã€ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼æ¤œå‡ºï¼‰
                    lines = content.split('\n')
                    issues_total += len([l for l in lines if len(l) > 120])  # é•·ã„è¡Œ
                    
                except Exception:
                    continue
            
            analysis_time = time.perf_counter() - start_analysis
            
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['file_count'] = file_count
            benchmark_result['issues_found'] = issues_total
            benchmark_result['analysis_time'] = analysis_time
            benchmark_result['throughput'] = file_count / analysis_time if analysis_time > 0 else 0
            
            print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
            print(f"ğŸ“Š ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {benchmark_result['throughput']:.1f} files/sec")
            print(f"âš¡ ç·å®Ÿè¡Œæ™‚é–“: {benchmark_result['execution_time']:.3f}ç§’")
            
            return benchmark_result
            
        except ValueError as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = f'Path validation error: {str(e)}'
            print(f"âŒ ãƒ‘ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return benchmark_result
        except (subprocess.TimeoutExpired, TimeoutError) as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = 'Execution timeout'
            print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")
            return benchmark_result
        except Exception as e:
            benchmark_result = self.end_measurement(measurement)
            benchmark_result['error'] = f'Unexpected error: {str(e)}'
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            return benchmark_result
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸš€ VIBEZENåŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print("=" * 60)
        
        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
        test_projects = [
            "/mnt/c/Users/tky99/dev/vibezen",
            "/mnt/c/Users/tky99/dev/techbookanalytics", 
            "/mnt/c/Users/tky99/dev/narou_converter",
            "/mnt/c/Users/tky99/dev/techbookfest_scraper"
        ]
        
        all_results = {
            'benchmark_time': datetime.now().isoformat(),
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_total_gb': psutil.virtual_memory().total / 1024**3,
                'python_version': sys.version
            },
            'results': {}
        }
        
        for project in test_projects:
            if not Path(project).exists():
                continue
                
            project_name = Path(project).name
            print(f"\nğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_name}")
            print("-" * 40)
            
            # å“è³ªãƒã‚§ãƒƒã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            quality_result = self.benchmark_quality_check(project)
            
            # VZã‚³ãƒãƒ³ãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯  
            vz_result = self.benchmark_vz_command(project)
            
            # å¤§è¦æ¨¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            large_result = self.benchmark_large_project(project)
            
            all_results['results'][project_name] = {
                'quality_check': quality_result,
                'vz_command': vz_result,
                'large_project': large_result
            }
        
        return all_results
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """çµæœä¿å­˜"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"vibezen_benchmark_{timestamp}.json"
        
        filepath = Path("/mnt/c/Users/tky99/dev/vibezen/benchmarks") / filename
        filepath.parent.mkdir(exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ä¿å­˜: {filepath}")
        return filepath
    
    def analyze_results(self, results: Dict[str, Any]):
        """çµæœåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)
        
        total_execution_time = 0
        total_memory_peak = 0
        total_throughput = 0
        project_count = 0
        
        for project_name, project_results in results['results'].items():
            print(f"\nğŸ” {project_name}:")
            
            if 'quality_check' in project_results:
                qc = project_results['quality_check']
                if 'execution_time' in qc:
                    print(f"  å“è³ªãƒã‚§ãƒƒã‚¯: {qc['execution_time']:.3f}ç§’")
                    total_execution_time += qc['execution_time']
            
            if 'large_project' in project_results:
                lp = project_results['large_project']
                if 'throughput' in lp:
                    print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {lp['throughput']:.1f} files/sec")
                    total_throughput += lp['throughput']
                    project_count += 1
        
        print(f"\nğŸ“ˆ å…¨ä½“ã‚µãƒãƒªãƒ¼:")
        print(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {total_execution_time / max(1, len(results['results'])):.3f}ç§’")
        print(f"  å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_throughput / max(1, project_count):.1f} files/sec")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        avg_throughput = total_throughput / max(1, project_count)
        if avg_throughput > 50:
            grade = "ğŸŒŸ A"
            comment = "å„ªç§€ï¼é«˜é€Ÿå‡¦ç†ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™"
        elif avg_throughput > 30:
            grade = "âœ… B"
            comment = "è‰¯å¥½ã€‚å®Ÿç”¨çš„ãªå‡¦ç†é€Ÿåº¦ã§ã™"
        elif avg_throughput > 15:
            grade = "ğŸŸ¡ C"
            comment = "è¦æ³¨æ„ã€‚æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™"
        else:
            grade = "ğŸ”´ D"
            comment = "å•é¡Œã‚ã‚Šã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ãŒå¿…è¦ã§ã™"
        
        print(f"\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡: {grade}")
        print(f"   {comment}")
        
        return {
            'grade': grade,
            'avg_throughput': avg_throughput,
            'total_execution_time': total_execution_time
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    benchmark = VIBEZENBenchmark()
    
    # åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    results = benchmark.run_comprehensive_benchmark()
    
    # çµæœä¿å­˜
    filepath = benchmark.save_results(results)
    
    # åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
    analysis = benchmark.analyze_results(results)
    
    print(f"\nğŸ‰ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼")
    print(f"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {filepath}")
    
    return analysis

if __name__ == "__main__":
    main()