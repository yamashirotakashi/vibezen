#!/usr/bin/env python3
"""
VIBEZEN è¶…é«˜é€Ÿå“è³ªãƒã‚§ãƒƒã‚«ãƒ¼
æœ€å¤§é™ã®æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€Ÿå“è³ªãƒã‚§ãƒƒã‚¯

æ©Ÿèƒ½:
- ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«è§£æ
- ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å¤§ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ä»˜ããƒªã‚½ãƒ¼ã‚¹ç®¡ç†

ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£:
- å…¥åŠ›ãƒ‘ã‚¹ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- å…·ä½“çš„ä¾‹å¤–å‡¦ç†
- ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
"""

import sys
import os
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import json
import mmap

class UltraFastQualityChecker:
    """è¶…é«˜é€Ÿå“è³ªãƒã‚§ãƒƒã‚«ãƒ¼"""
    
    # å®šæ•°å®šç¾©
    MAX_CPU_CORES = 8
    MAX_FILE_SIZE_BYTES = 500_000  # 500KB
    MAX_FILES_PER_BATCH = 50
    QUICK_SCAN_THRESHOLD_LINES = 100
    LONG_LINE_THRESHOLD = 120
    MAX_ISSUES_PER_FILE = 20
    SAMPLING_INTERVAL = 10
    MAX_SAMPLING_ISSUES = 10
    BATCH_TIMEOUT_SECONDS = 10.0
    EXECUTOR_TIMEOUT_SECONDS = 30.0
    
    def __init__(self):
        self.cpu_cores = min(cpu_count(), self.MAX_CPU_CORES)
        self.max_file_size = self.MAX_FILE_SIZE_BYTES
        self.max_files_per_batch = self.MAX_FILES_PER_BATCH
        self.quick_scan_threshold = self.QUICK_SCAN_THRESHOLD_LINES
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
        self.exclude_patterns = {
            '__pycache__',
            '.git',
            'node_modules',
            '.venv',
            'venv',
            'migrations',
            'test_',
            '_test',
            '.pyc'
        }
        
        # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«å
        self.exclude_files = {
            '__init__.py',
            'setup.py',
            'conftest.py'
        }
    
    def should_skip_file(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®šï¼ˆè¶…é«˜é€Ÿï¼‰"""
        # ãƒ‘ã‚¹æ–‡å­—åˆ—ã§é«˜é€Ÿåˆ¤å®š
        path_str = str(file_path)
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        for pattern in self.exclude_patterns:
            if pattern in path_str:
                return True
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åãƒã‚§ãƒƒã‚¯
        if file_path.name in self.exclude_files:
            return True
        
        # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        try:
            if file_path.stat().st_size > self.max_file_size:
                return True
        except (OSError, IOError, PermissionError) as e:
            return True
            
        return False
    
    def quick_file_scan(self, file_path: Path) -> Dict[str, Any]:
        """è¶…é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³"""
        start_time = time.perf_counter()
        
        try:
            # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã«ã‚ˆã‚‹é«˜é€Ÿèª­ã¿è¾¼ã¿
            with open(file_path, 'rb') as f:
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    content = mm.read().decode('utf-8', errors='ignore')
            
            lines = content.split('\n')
            total_lines = len(lines)
            
            # ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒ³ãƒ¢ãƒ¼ãƒ‰
            if total_lines < self.quick_scan_threshold:
                issues = self._full_scan(lines, file_path.name)
            else:
                issues = self._sampling_scan(lines, file_path.name)
            
            code_lines = sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
            
            return {
                'file': file_path.name,
                'total_lines': total_lines,
                'code_lines': code_lines,
                'issues': issues,
                'analysis_time': time.perf_counter() - start_time,
                'scan_mode': 'quick' if total_lines < self.quick_scan_threshold else 'sampling'
            }
            
        except (UnicodeDecodeError, IOError, PermissionError) as e:
            return {
                'file': file_path.name,
                'error': f'File access error: {str(e)}',
                'analysis_time': time.perf_counter() - start_time
            }
        except Exception as e:
            return {
                'file': file_path.name,
                'error': f'Unexpected error: {str(e)}',
                'analysis_time': time.perf_counter() - start_time
            }
    
    def _full_scan(self, lines: List[str], filename: str) -> List[Dict[str, Any]]:
        """å®Œå…¨ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå°ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰"""
        issues = []
        
        for i, line in enumerate(lines, 1):
            # é•·ã„è¡Œãƒã‚§ãƒƒã‚¯
            if len(line) > self.LONG_LINE_THRESHOLD:
                issues.append({
                    'type': 'long_line',
                    'line': i,
                    'severity': 'low',
                    'description': f'é•·ã„è¡Œ ({len(line)}æ–‡å­—)'
                })
            
            # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            if any(char.isdigit() for char in line) and not line.strip().startswith('#'):
                import re
                numbers = re.findall(r'\b\d{2,}\b', line)
                for num in numbers:
                    if num not in ['10', '20', '50', '100', '200', '500', '1000']:
                        issues.append({
                            'type': 'magic_number',
                            'line': i,
                            'severity': 'medium',
                            'description': f'ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ {num}'
                        })
                        if len(issues) > self.MAX_ISSUES_PER_FILE:
                            return issues
        
        return issues
    
    def _sampling_scan(self, lines: List[str], filename: str) -> List[Dict[str, Any]]:
        """ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå¤§ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰"""
        issues = []
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        sample_lines = [(i, line) for i, line in enumerate(lines, 1) if i % self.SAMPLING_INTERVAL == 0]
        
        for i, line in sample_lines:
            # é•·ã„è¡Œãƒã‚§ãƒƒã‚¯ã®ã¿
            if len(line) > self.LONG_LINE_THRESHOLD:
                issues.append({
                    'type': 'long_line',
                    'line': i,
                    'severity': 'low',
                    'description': f'é•·ã„è¡Œã‚µãƒ³ãƒ—ãƒ« ({len(line)}æ–‡å­—)'
                })
                
            if len(issues) > self.MAX_SAMPLING_ISSUES:
                break
        
        return issues
    
    def collect_files_fast(self, project_path: str, max_files: int = 100) -> List[Path]:
        """é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åé›†"""
        project_dir = Path(project_path)
        python_files = []
        
        # å†å¸°çš„æ¤œç´¢ï¼ˆåˆ¶é™ä»˜ãï¼‰
        for py_file in project_dir.rglob("*.py"):
            if self.should_skip_file(py_file):
                continue
                
            python_files.append(py_file)
            
            if len(python_files) >= max_files:
                break
        
        return python_files
    
    def process_file_batch(self, file_batch: List[Path]) -> List[Dict[str, Any]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒãƒå‡¦ç†"""
        results = []
        for file_path in file_batch:
            result = self.quick_file_scan(file_path)
            results.append(result)
        return results
    
    def ultra_fast_check(self, project_path: str, max_files: int = 100) -> Dict[str, Any]:
        """è¶…é«˜é€Ÿå“è³ªãƒã‚§ãƒƒã‚¯"""
        start_time = time.perf_counter()
        
        print(f"ğŸš€ è¶…é«˜é€Ÿå“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹: {Path(project_path).name}")
        print("=" * 50)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åé›†
        file_collection_start = time.perf_counter()
        python_files = self.collect_files_fast(project_path, max_files)
        file_collection_time = time.perf_counter() - file_collection_start
        
        print(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«åé›†: {len(python_files)}ä»¶ ({file_collection_time:.3f}ç§’)")
        
        if not python_files:
            print("âš ï¸ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {'error': 'No Python files found'}
        
        # ãƒãƒƒãƒåˆ†å‰²
        batches = [
            python_files[i:i + self.max_files_per_batch] 
            for i in range(0, len(python_files), self.max_files_per_batch)
        ]
        
        print(f"âš¡ ä¸¦åˆ—å‡¦ç†: {len(batches)}ãƒãƒƒãƒ Ã— {self.cpu_cores}ã‚³ã‚¢")
        
        # ä¸¦åˆ—å‡¦ç†
        all_results = []
        processing_start = time.perf_counter()
        
        with ProcessPoolExecutor(max_workers=self.cpu_cores) as executor:
            future_to_batch = {
                executor.submit(process_batch_worker, batch): batch 
                for batch in batches
            }
            
            for future in as_completed(future_to_batch, timeout=self.EXECUTOR_TIMEOUT_SECONDS):
                try:
                    batch_results = future.result(timeout=self.BATCH_TIMEOUT_SECONDS)
                    all_results.extend(batch_results)
                    print(f"âœ… ãƒãƒƒãƒå®Œäº†: {len(batch_results)}ãƒ•ã‚¡ã‚¤ãƒ«")
                except TimeoutError:
                    print(f"â° ãƒãƒƒãƒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 5ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ")
                except Exception as e:
                    print(f"âŒ ãƒãƒƒãƒã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        processing_time = time.perf_counter() - processing_start
        total_time = time.perf_counter() - start_time
        
        # çµæœé›†è¨ˆ
        total_lines = 0
        total_issues = 0
        successful_files = 0
        
        for result in all_results:
            if 'error' not in result:
                total_lines += result.get('code_lines', 0)
                total_issues += len(result.get('issues', []))
                successful_files += 1
        
        throughput = len(python_files) / total_time if total_time > 0 else 0
        
        # çµæœè¡¨ç¤º
        self._print_ultra_fast_summary(
            successful_files, total_lines, total_issues, 
            total_time, processing_time, throughput
        )
        
        return {
            'total_files': len(python_files),
            'successful_files': successful_files,
            'total_lines': total_lines,
            'total_issues': total_issues,
            'total_time': total_time,
            'processing_time': processing_time,
            'throughput': throughput,
            'results': all_results
        }
    
    def _print_ultra_fast_summary(self, successful_files: int, total_lines: int, 
                                  total_issues: int, total_time: float, 
                                  processing_time: float, throughput: float):
        """è¶…é«˜é€Ÿçµæœã‚µãƒãƒªãƒ¼"""
        print("\n" + "=" * 50)
        print("âš¡ è¶…é«˜é€Ÿå“è³ªãƒã‚§ãƒƒã‚¯çµæœ")
        print("=" * 50)
        
        issue_density = (total_issues / total_lines) * 1000 if total_lines > 0 else 0
        
        print(f"\nğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"  ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
        print(f"  å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} files/sec")
        print(f"  æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«: {successful_files}")
        
        print(f"\nğŸ“Š å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        print(f"  ç·ã‚³ãƒ¼ãƒ‰è¡Œæ•°: {total_lines:,}è¡Œ")
        print(f"  æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {total_issues}ä»¶")
        print(f"  å•é¡Œå¯†åº¦: {issue_density:.1f}ä»¶/1000è¡Œ")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        if throughput > 50:
            perf_grade = "ğŸš€ è¶…é«˜é€Ÿ"
            perf_comment = "ç›®æ¨™é”æˆï¼"
        elif throughput > 20:
            perf_grade = "âš¡ é«˜é€Ÿ"
            perf_comment = "å„ªç§€ãªæ€§èƒ½"
        elif throughput > 10:
            perf_grade = "ğŸŸ¡ æ¨™æº–"
            perf_comment = "å®Ÿç”¨çš„"
        else:
            perf_grade = "ğŸ”´ ä½é€Ÿ"
            perf_comment = "è¦æ”¹å–„"
        
        print(f"\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡: {perf_grade}")
        print(f"   {perf_comment} ({throughput:.1f} files/sec)")

def process_batch_worker(file_batch: List[Path]) -> List[Dict[str, Any]]:
    """
    ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ç”¨ãƒãƒƒãƒå‡¦ç†é–¢æ•°
    
    Args:
        file_batch: å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        
    Returns:
        ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
        
    Note:
        ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚·ãƒ³ã‚°ç’°å¢ƒã§å®‰å…¨ã«å®Ÿè¡Œã•ã‚Œã‚‹ã‚ˆã†è¨­è¨ˆ
    """
    try:
        checker = UltraFastQualityChecker()
        return checker.process_file_batch(file_batch)
    except Exception as e:
        # ãƒ—ãƒ­ã‚»ã‚¹é–“ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ç©ºã®çµæœã‚’è¿”ã™
        return [{'file': 'batch_error', 'error': f'Batch processing failed: {str(e)}'}]

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    if len(sys.argv) > 1:
        project_path = sys.argv[1]
        max_files = int(sys.argv[2]) if len(sys.argv) > 2 else 100
    else:
        project_path = os.getcwd()
        max_files = 100
    
    checker = UltraFastQualityChecker()
    results = checker.ultra_fast_check(project_path, max_files)
    
    # çµæœä¿å­˜
    if 'error' not in results:
        output_file = Path(project_path) / "ultra_fast_quality_report.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            serializable_results = {
                'total_files': results['total_files'],
                'successful_files': results['successful_files'],
                'total_lines': results['total_lines'],
                'total_issues': results['total_issues'],
                'total_time': results['total_time'],
                'throughput': results['throughput'],
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
            }
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ çµæœä¿å­˜: {output_file}")
    
    return results

if __name__ == "__main__":
    main()